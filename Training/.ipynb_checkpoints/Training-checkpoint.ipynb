{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark NLP Annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_file = '/home/user/elicon/data/Test_Data/Nepal_Quake/json/batch1/'\n",
    "df = spark.read.option(\"header\",\"true\").csv(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#data = spark. \\\n",
    "#        read. \\\n",
    "#        parquet(\"../sentiment.parquet\"). \\\n",
    "#        limit(1000)\n",
    "df.cache()\n",
    "df.count()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sparknlp.annotator import *\n",
    "from sparknlp.common import *\n",
    "from sparknlp.base import *\n",
    "from pyspark.ml import Pipeline\n",
    "### Define the dataframe                                                               \n",
    "document_assembler = DocumentAssembler() \\\n",
    "            .setInputCol(\"tweet\")\n",
    "assembled = document_assembler.transform(df)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer() \\\n",
    "  .setInputCols([\"document\"]) \\\n",
    "  .setOutputCol(\"token\") \\\n",
    "  #.addInfixPattern(\"(\\p{L}+)(n't\\b)\")\n",
    "tokenized = tokenizer.transform(assembled)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "extractor = EntityExtractor()\\\n",
    "  .setEntities(\"file://\" + os.getcwd() + \"/entities.txt\")\\\n",
    "  .setInputCols([\"token\"])\\\n",
    "  .setOutputCol(\"entities\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stemmer = Stemmer() \\\n",
    "  .setInputCols([\"token\"]) \\\n",
    "  .setOutputCol(\"stem\")\n",
    "stemmed = stemmer.transform(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Spell Checker\n",
    "spell_checker = NorvigSweetingApproach() \\\n",
    "            .setInputCols([\"stem\"]) \\\n",
    "            .setOutputCol(\"spell\")\n",
    "checked = spell_checker.fit(stemmed).transform(stemmed)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "token_assembler = TokenAssembler() \\\n",
    ".setInputCols([\"spell\"]) \\\n",
    ".setOutputCol(\"assembled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "finisher = Finisher() \\\n",
    "    .setInputCols([\"assembled\"]) \\\n",
    "    .setIncludeKeys(True) \\\n",
    "    .setCleanAnnotations(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[\n",
    "    document_assembler,\n",
    "    tokenizer,\n",
    "    stemmer,\n",
    "    spell_checker,\n",
    "    token_assembler,\n",
    "    finisher\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spelled_data = pipeline.fit(df).transform(df)\n",
    "spelled_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spelled_data.select('assembled').show(2, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple feature training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load the Dataset \n",
    "from pyspark.sql import DataFrame\n",
    "#data_file = '/home/user/elicon/data/Test_Data/California_Quake/json/california_earthquake_rawTweets.csv'\n",
    "#data_file = '/home/user/elicon/data/Test_Data/California_Quake/unlabelled/retweeted/processed/napa_fin.csv'\n",
    "#data_file = '/home/user/elicon/data/Test_Data/California_Quake/datasets/california_test.csv'\n",
    "data_file = '/home/user/elicon/data/train.csv'\n",
    "df = spark.read.option(\"header\",\"true\").csv(data_file)\n",
    "df = df.select(df.tweet_text,df.label.cast(\"double\").alias(\"label\"))\n",
    "df = df.dropna()\n",
    "df.createOrReplaceTempView(\"tweets\")\n",
    "tweet_label = spark.sql(\"SELECT tweet_text, label FROM tweets\")\n",
    "\n",
    "tweet_text_only = spark.sql(\"SELECT tweet_text FROM tweets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Tokenise tweets\n",
    "from pyspark.ml.feature import Tokenizer\n",
    "#from pyspark.sql.functions import col, udf\n",
    "#from pyspark.sql.types import IntegerType\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"tweet_text\", outputCol=\"words\")\n",
    "#countTokens = udf(lambda words: len(words), IntegerType())\n",
    "#tokenized = tokenizer.transform(tweet_label)\n",
    "#tokenized = tokenized.withColumn(\"tokens\", countTokens(col(\"words\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover\n",
    "# stop words\n",
    "add_stopwords = [\"amp\",\"rt\",\"ca\",\"pg\",\"ga\",\"cb\",\"ap\",\"sce\",\"st\"] # standard stop words\n",
    "stopwordsRemover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\").setStopWords(add_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 3-Gram for context feature\n",
    "from pyspark.ml.feature import NGram\n",
    "\n",
    "ngram = NGram(n=3, inputCol=\"filtered\", outputCol=\"ngrams\")\n",
    "#ngrams = ngram.transform(tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use HashingTF Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Hashing TF feature extraction\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "from pyspark.ml.linalg import Vector as MLVector, Vectors as MLVectors\n",
    "from pyspark.mllib.linalg import Vector as MLLibVector, Vectors as MLLibVectors\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "hashingTF = HashingTF(inputCol=\"ngrams\", outputCol=\"rawFeatures\", numFeatures=50000)\n",
    "#featurizedData = hashingTF.transform(ngrams)\n",
    "# alternatively, CountVectorizer can also be used to get term frequency vectors\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "#idfModel = idf.fit(featurizedData)\n",
    "#tfidfData = idfModel.transform(featurizedData)\n",
    "#assembler = VectorAssembler(\n",
    "#    inputCols=[\"features\", \"tokens\"],\n",
    "#    outputCol=\"union_features\")\n",
    "#assembledData = assembler.transform(tfidfData)\n",
    "#pairs = assembledData.select(\"label\",\"features\").rdd\n",
    "#pairs = tfidfData.select(\"label\",\"features\").rdd\n",
    "#data = pairs.map(lambda x: LabeledPoint(x[0], as_mllib(x[1])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert to labeled vectors\n",
    "from pyspark.ml import linalg as ml_linalg\n",
    "\n",
    "def as_mllib(v):\n",
    "    if isinstance(v, ml_linalg.SparseVector):\n",
    "        return MLLibVectors.sparse(v.size, v.indices, v.values)\n",
    "    elif isinstance(v, ml_linalg.DenseVector):\n",
    "        return MLLibVectors.dense(v.toArray())\n",
    "    else:\n",
    "        raise TypeError(\"Unsupported type: {0}\".format(type(v)))        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Word2Vec Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Word2Vec\n",
    "# Learn a mapping from words to Vectors.\n",
    "word2Vec = Word2Vec(vectorSize=2000, minCount=5, inputCol=\"ngrams\", outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "#pipeline = Pipeline(stages=[tokenizer,stopwordsRemover, ngram, word2Vec])# Fit the pipeline to training documents.\n",
    "pipeline = Pipeline(stages=[tokenizer, stopwordsRemover, ngram, hashingTF, idf])# Fit the pipeline to training documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipelineFit = pipeline.fit(tweet_label)\n",
    "dataset = pipelineFit.transform(tweet_label)\n",
    "### Randomly split data into training and test sets. set seed for reproducibility\n",
    "(trainingData, testData) = dataset.randomSplit([0.7, 0.3], seed = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokenized = tokenizer.transform(tweet_label)\n",
    "stopword = stopwordsRemover.transform(tokenized)\n",
    "ngrams = ngram.transform(stopword)\n",
    "featurizedData = hashingTF.transform(ngrams)\n",
    "idfModel = idf.fit(featurizedData)\n",
    "tfidfData = idfModel.transform(featurizedData)\n",
    "pairs = tfidfData.select(\"label\",\"features\").rdd\n",
    "data = pairs.map(lambda x: LabeledPoint(x[0], as_mllib(x[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "splits = data.randomSplit([0.6, 0.4], 1234)\n",
    "mllib_trainingData = splits[0]\n",
    "mllib_testData = splits[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "# Build the model\n",
    "lr = LogisticRegression(maxIter=50, regParam=0.3, elasticNetParam=0)\n",
    "\n",
    "# Train model with Training Data\n",
    "lrModel = lr.fit(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Run Results on Training Data\n",
    "trainingSummary = lrModel.summary\n",
    "print(\"Training: Area Under ROC: \" + str(trainingSummary.areaUnderROC))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Predict on Test Data\n",
    "predictions = lrModel.transform(testData)\n",
    "# Evaluate model\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\n",
    "print(\"Test: Area Under ROC: \" + str(evaluator.evaluate(predictions, {evaluator.metricName: \"areaUnderROC\"})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "beta = np.sort(lrModel.coefficients)\n",
    "\n",
    "plt.plot(beta)\n",
    "plt.ylabel('Beta Coefficients')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Extract the summary from the returned LogisticRegressionModel instance trained\n",
    "trainingSummary = lrModel.summary\n",
    "\n",
    "# Obtain the objective per iteration\n",
    "objectiveHistory = trainingSummary.objectiveHistory\n",
    "plt.plot(objectiveHistory)\n",
    "plt.ylabel('Objective Function')\n",
    "plt.xlabel('Iteration')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Obtain the receiver-operating characteristic as a dataframe and areaUnderROC.\n",
    "print(\"areaUnderROC: \" + str(trainingSummary.areaUnderROC))\n",
    "\n",
    "#trainingSummary.roc.show(n=10, truncate=15)\n",
    "roc = trainingSummary.roc.toPandas()\n",
    "plt.plot(roc['FPR'],roc['TPR'])\n",
    "plt.ylabel('False Positive Rate')\n",
    "plt.xlabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pr = trainingSummary.pr.toPandas()\n",
    "plt.plot(pr['recall'],pr['precision'])\n",
    "plt.ylabel('Precision')\n",
    "plt.xlabel('Recall')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set the model threshold to maximize F-Measure\n",
    "#trainingSummary.fMeasureByThreshold.show(n=10, truncate = 15)\n",
    "f = trainingSummary.fMeasureByThreshold.toPandas()\n",
    "plt.plot(f['threshold'],f['F-Measure'])\n",
    "plt.ylabel('F-Measure')\n",
    "plt.xlabel('Threshold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "# Create ParamGrid for Cross Validation\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(lr.regParam, [0.1, 0.3, 0.5]) # regularization parameter\n",
    "             .addGrid(lr.elasticNetParam, [0.0, 0.1, 0.2]) # Elastic Net Parameter (Ridge = 0)\n",
    "#            .addGrid(model.maxIter, [10, 20, 50]) #Number of iterations\n",
    "#            .addGrid(idf.numFeatures, [10, 100, 1000]) # Number of features\n",
    "             .build())\n",
    "\n",
    "# Create 5-fold CrossValidator\n",
    "cv = CrossValidator(estimator=lr, \\\n",
    "                    estimatorParamMaps=paramGrid, \\\n",
    "                    evaluator=evaluator, \\\n",
    "                    numFolds=5)\n",
    "\n",
    "# Run cross validations\n",
    "cvModel = cv.fit(trainingData)\n",
    "# this will likely take a fair amount of time because of the amount of models that we're creating and testing\n",
    "\n",
    "# Use test set here so we can measure the accuracy of our model on new data\n",
    "predictions = cvModel.transform(testData)\n",
    "\n",
    "# cvModel uses the best model found from the Cross Validation\n",
    "# Evaluate best model\n",
    "print(\"Test: Area Under ROC: \" + str(evaluator.evaluate(predictions, {evaluator.metricName: \"areaUnderROC\"})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipeline.save(\"target/tmp/HashingTF_Binary_pipeline_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lrModel.save(\"target/tmp/logisticRegression_Binary_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Linear SVM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LinearSVC\n",
    "\n",
    "# Build the model\n",
    "svm = LinearSVC(maxIter=100, regParam=0.01)\n",
    "\n",
    "# Train model with Training Data\n",
    "svmModel = svm.fit(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: Area Under ROC: 0.599666954484\n",
      "Test: Area Under ROC: 0.604154135354\n"
     ]
    }
   ],
   "source": [
    "# Use test set here so we can measure the accuracy of our model on new data\n",
    "predictions = svmModel.transform(testData)\n",
    "\n",
    "# Evaluate model\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\n",
    "print(\"Test: Area Under ROC: \" + str(evaluator.evaluate(predictions, {evaluator.metricName: \"areaUnderROC\"})))\n",
    "print(\"Test: Area Under PR: \" + str(evaluator.evaluate(predictions, {evaluator.metricName: \"areaUnderPR\"})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "svmModel.save(\"target/tmp/SVM_Binary_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "# Create an initial RandomForest model.\n",
    "rf = RandomForestClassifier(labelCol=\"label\", \\\n",
    "                            featuresCol=\"features\", \\\n",
    "                            numTrees = 100, \\\n",
    "                            maxDepth = 4, \\\n",
    "                            maxBins = 32)\n",
    "\n",
    "# Train model with Training Data\n",
    "rfModel = rf.fit(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\n",
    "print(\"Test: Area Under ROC: \" + str(evaluator.evaluate(predictions, {evaluator.metricName: \"areaUnderROC\"})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "# Create ParamGrid for Cross Validation\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(rf.numTrees, [50, 100, 200]) # number of trees\n",
    "             .addGrid(rf.maxDepth, [3, 4, 5]) # maximum depth\n",
    "#            .addGrid(rf.maxBins, [24, 32, 40]) #Number of bins\n",
    "             .build())\n",
    "\n",
    "# Create 5-fold CrossValidator\n",
    "cv = CrossValidator(estimator=rf, \\\n",
    "                    estimatorParamMaps=paramGrid, \\\n",
    "                    evaluator=evaluator, \\\n",
    "                    numFolds=5)\n",
    "\n",
    "# Run cross validations\n",
    "cvModel = cv.fit(trainingData)\n",
    "\n",
    "# Use test set here so we can measure the accuracy of our model on new data\n",
    "predictions = cvModel.transform(testData)\n",
    "\n",
    "# cvModel uses the best model found from the Cross Validation\n",
    "# Evaluate best model\n",
    "print(\"Test: Area Under ROC: \" + str(evaluator.evaluate(predictions, {evaluator.metricName: \"areaUnderROC\"})))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import GBTClassifier\n",
    "\n",
    "# Train a GBT model.\n",
    "gbt = GBTClassifier(labelCol=\"label\", \\\n",
    "                    featuresCol=\"features\", \\\n",
    "                    maxIter=10 \\\n",
    "                    )\n",
    "\n",
    "# Train model.  This also runs the indexers.\n",
    "model = gbt.fit(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make predictions.\n",
    "predictions = model.transform(testData)\n",
    "\n",
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"label\", \\\n",
    "                                          rawPredictionCol=\"prediction\")\n",
    "print(\"Test: Area Under ROC: \" + str(evaluator.evaluate(predictions, {evaluator.metricName: \"areaUnderROC\"})))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import NaiveBayes\n",
    "\n",
    "# create the trainer and set its parameters\n",
    "nb = NaiveBayes(smoothing=1, modelType=\"multinomial\")\n",
    "\n",
    "# train the model\n",
    "model = nb.fit(trainingData)\n",
    "\n",
    "# select example rows to display.\n",
    "predictions = model.transform(testData)\n",
    "\n",
    "# compute accuracy on the test set\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\")\n",
    "print(\"Test: Area Under ROC: \" + str(evaluator.evaluate(predictions, {evaluator.metricName: \"areaUnderROC\"})))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ## Linear SVM\n",
    "from pyspark.mllib.classification import SVMWithSGD, SVMModel \n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "\n",
    "# Build the model\n",
    "model = SVMWithSGD.train(mllib_trainingData, iterations=100)\n",
    "\n",
    "# Evaluating the model on training data\n",
    "labelsAndPreds = mllib_trainingData.map(lambda p: (p.label, model.predict(p.features)))\n",
    "trainErr = labelsAndPreds.filter(lambda (v, p): v != p).count() / float(mllib_trainingData.count())\n",
    "print(\"Training Error = \" + str(trainErr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scoreAndLabels = mllib_testData.map(lambda p: (p.label, model.predict(p.features)))\n",
    "srt = scoreAndLabels.map(lambda xs: [float(x) for x in xs])\n",
    "nest = srt.map(lambda l : tuple(l) )\n",
    "metrics = BinaryClassificationMetrics(nest)\n",
    "au_ROC = metrics.areaUnderROC\n",
    "au_PR = metrics.areaUnderPR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "au_PR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "pipeline = Pipeline(stages=[tokenizer, ngram, hashingTF, idf])\n",
    "pipeline.save(\"target/tmp/HashingTF_Binary_pipeline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipeModel = pipeline.fit(tweet_label)\n",
    "pipeModel.save(\"target/tmp/HashingTF_Binary_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save(sc, \"target/tmp/SVMWithSGD_Binary_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Multi-feature training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def search(text, search_word):\n",
    "    word = r\"\\W*([\\w]+)\"\n",
    "    groups = re.search(r\"{}\\W*{}{}\".format(word, search_word, word), text).groups()\n",
    "    return list(groups[:1]),list(groups[1:])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t = \"fire erupt quak rock california wine countri\"\n",
    "tuple1 = search(t, \"quak\")\n",
    "contents = list(tuple1)\n",
    "content = [item for sub in contents for item in sub]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"label\", \"tokens\"],\n",
    "    outputCol=\"union_features\")\n",
    "assembledData = assembler.transform(ngrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "indexer = StringIndexer(inputCol=\"ngrams\", outputCol=\"ngrams_numeric\").fit(ngrams)\n",
    "indexed_df = indexer.transform(ngrams)\n",
    "#indexed_df.drop(\"bar\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
