{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Resourses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "link = 'http://crisisnlp.qcri.org/data/lrec2016/labeled_cf/CrisisNLP_labeled_data_crowdflower_V1.2.zip'\n",
    "dest = '../../data/Datasets'\n",
    "datasets = 'CrisisNLP_labeled_data_crowdflower_V1.2.zip'    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!cd $dest && wget $link && unzip $datasets && rm -r $datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "original_file = '2014_california_eq.csv'\n",
    "original_folder = '../../data/Datasets/CrisisNLP_labeled_data_crowdflower.DS_Store/2014_California_Earthquake'\n",
    "dest_folder = '../../data/Datasets/Training_Data/California-Quake'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!cd $dest_folder && awk -F ',' '{print>$1}' $original_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!cd $original_folder && awk -F ',' '{print > (\"data\"$0\".csv\")}' $original_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!cat $original_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!java -classpath TweetsRetrieval-1.2-jar-with-dependencies.jar qa.qcri.tweetsretrieval.TweetsRetrievalTool $original_file "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import some nltk functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import csv\n",
    "import string\n",
    "import re\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import *\n",
    "stemmer = PorterStemmer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def expand_around_chars(text, characters):\n",
    "    for char in characters:\n",
    "        text = text.replace(char, ' ' + char + ' ')\n",
    "    return text\n",
    "\n",
    "def strip_quotations_newline(text):\n",
    "    clean_tweet = ' '.join(text.split())\n",
    "    clean_tweet = clean_tweet.decode('unicode_escape').encode('ascii','ignore')\n",
    "    clean_tweet = clean_tweet.replace('\",\\'','')\n",
    "    clean_tweet = re.sub(r'''(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\".,<>?«»“”‘’]))''', \"\", clean_tweet)\n",
    "    clean_tweet = re.sub(r'''(@[A-Za-z0-9]+)''', \"\", clean_tweet)\n",
    "    clean_tweet = re.sub(\"([0-9]+)\", \"\", clean_tweet)\n",
    "    return clean_tweet\n",
    "\n",
    "def split_text(text):\n",
    "    text = strip_quotations_newline(text)\n",
    "    text = expand_around_chars(text, '\\/\".,()&[]{}:;!-_\\'')\n",
    "    splitted_text = text.split(' ')\n",
    "    cleaned_text = [x for x in splitted_text if len(x) > 1]\n",
    "    text_lowercase = [x.lower() for x in cleaned_text]\n",
    "    text_stemmed = [stemmer.stem(x) for x in text_lowercase]\n",
    "    text_stemmed = [x.decode('unicode_escape').encode('ascii','ignore') for x in text_stemmed]\n",
    "    text_filtered = [x for x in text_stemmed if not x in stop_words]    \n",
    "    return text_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JSON to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[Errno 20] Not a directory: '/home/user/tweets.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-edf11f551b49>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#path = '/home/user/elicon/data/Test_Data/Nepal_Quake/json/batch2'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/home/user/tweets.json'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0minput_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"*.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 20] Not a directory: '/home/user/tweets.json'"
     ]
    }
   ],
   "source": [
    "#path = '/home/user/elicon/data/Test_Data/Nepal_Quake/json/batch2'\n",
    "path = '/home/user/tweets.json'\n",
    "os.chdir(path)\n",
    "\n",
    "input_files = glob.glob(\"*.csv\")\n",
    "for finput in input_files:\n",
    "    with open(finput) as fin:\n",
    "        with open(( finput.rsplit( \".\", 1 )[ 0 ] ) + \".ttxt\", \"w+\") as fout:\n",
    "            for line in fin:\n",
    "                try:\n",
    "                    tweet = json.loads(line)\n",
    "                    id = tweet['created_at']\n",
    "                    if 'retweeted_status' in tweet:\n",
    "                        text = tweet['retweeted_status']['text']\n",
    "                    else:\n",
    "                        text = tweet['text']\n",
    "                    fout.write(str(id) + \",\" + str(text) + \"\\n\")\n",
    "                except:\n",
    "                    continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset = '/home/user/tweets_3.json'\n",
    "with open(dataset) as fin:\n",
    "        with open(( dataset.rsplit( \".\", 1 )[ 0 ] ) + \".csv\", \"w+\") as fout:\n",
    "            for line in fin:\n",
    "                try:\n",
    "                    tweet = json.loads(line)\n",
    "                    id = tweet['created_at']\n",
    "                    if 'retweeted_status' in tweet:\n",
    "                        text = tweet['retweeted_status']['text']\n",
    "                    else:\n",
    "                        text = tweet['text']\n",
    "                    fout.write(str(id) + \",\" + str(text) + \"\\n\")\n",
    "                except:\n",
    "                    continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Json to Csv keeping time and location information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "import json, glob, os\n",
    "#path = '/home/user/elicon/data/Test_Data/Chile_Quake/unlabelled/'\n",
    "path = '/home/user/datasets'\n",
    "os.chdir(path)\n",
    "os.getcwd()\n",
    "input_files = glob.glob(\"*.json\")\n",
    "\n",
    "#dataset = '/home/user/datasets/tweets.json'\n",
    "for finput in input_files: #iterates over all files in the directory ending in .csv\n",
    "    with open(finput) as fin:\n",
    "#with open(dataset) as fin:\n",
    "        with open(( finput.rsplit( \".\", 1 )[ 0 ] ) + \".csv\", \"w+\") as fout:\n",
    "            for line in fin:\n",
    "                try:\n",
    "                    tweet = json.loads(line)\n",
    "                    date = tweet['created_at']\n",
    "                    try:\n",
    "                        c = time.strptime(date.replace(\"+0000\",''), '%a %b %d %H:%M:%S %Y')\n",
    "                    except: \n",
    "                        print \"pb with tweet_gmttime\", date, line\n",
    "                        pass\n",
    "\n",
    "                    tweet_unixtime = int(time.mktime(c))\n",
    "                    date_time = datetime.fromtimestamp(tweet_unixtime).strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "                    if tweet['place']:\n",
    "                        location = tweet['place']\n",
    "                    else:\n",
    "                        location = tweet['user']['location']\n",
    "                    if 'retweeted_status' in tweet:\n",
    "                        text = tweet['retweeted_status']['text']\n",
    "                    else:\n",
    "                        text = tweet['text']\n",
    "                    filtered_text = ' '.join(split_text(text))    \n",
    "                    fout.write(str(date_time) + ',' + str(tweet_unixtime) + ',' + str(location) + \",\" + str(filtered_text) + \",\" + str(text) + \"\\n\")\n",
    "                except:\n",
    "                    continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://83.212.118.171:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fb7499b4510>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = '/home/user'\n",
    "os.chdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!awk -F ',' -v OFS=',' '{print $2}' tweets_3.csv > tweets_3.csvv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!sed 's/http[^ ]*//g' tweets_3.csvv > tweets_3.csv1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!awk '!seen[$0]++' tweets_3.csv1 > tweets_3.csv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!awk 'NF>=3' tweets_2.csv1 > tweets_2.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming,Stop words removing and other normalisations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "#path = '/home/user/elicon/data/Test_Data/Chile_Quake/unlabelled/'\n",
    "path = '/home/user/elicon/data/'\n",
    "os.chdir(path)\n",
    "os.getcwd()\n",
    "input_files = glob.glob(\"*.csv\")\n",
    "for finput in input_files: #iterates over all files in the directory ending in .csv\n",
    "    with open(finput) as fin:\n",
    "        with open(( finput.rsplit( \".\", 1 )[ 0 ] ) + \".csv1\", \"w+\") as fout:\n",
    "            for tweet in fin:\n",
    "                clean_doc = ' '.join(split_text(tweet))\n",
    "                json.dump(clean_doc, fout)\n",
    "                fout.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#finput = '/home/user/elicon/data/Test_Data/Chile_Quake/unlabelled/chile_quake_rawTweets.txt'\n",
    "#finput = '/home/user/elicon/data/Test_Data/California_Quake/datasets'\n",
    "finput = '/home/user/tweets.xcsv'\n",
    "with open(finput) as fin:\n",
    "        with open(( finput.rsplit( \".\", 1 )[ 0 ] ) + \".csv\", \"w+\") as fout:\n",
    "            fout.write('tweet_text,label\\n')\n",
    "            for tweet in fin:\n",
    "                clean_doc = ' '.join(split_text(tweet))\n",
    "                json.dump(clean_doc, fout)\n",
    "                fout.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add labels for multiclass classification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## add labels \n",
    "path = '/home/user/elicon/data/Training_Data/Nepal_Quake/labeled'\n",
    "os.chdir(path)\n",
    "#os.getcwd()\n",
    "input_files = glob.glob(\"*.csv\")\n",
    "for finput in input_files: #iterates over all files in the directory ending in .csv\n",
    "    with open(finput, 'r') as fin:\n",
    "        if finput == 'injured_or_dead_people.csv':\n",
    "            string_to_add = \", 0\"\n",
    "        elif finput == 'missing_trapped_or_found_people.csv':\n",
    "            string_to_add = \", 1\"\n",
    "        elif finput == 'displaced_people_and_evacuations.csv':\n",
    "            string_to_add = \", 2\"\n",
    "        elif finput == 'infrastructure_and_utilities_damage.csv':\n",
    "            string_to_add = \", 3\"\n",
    "        elif finput == 'donation_needs_or_offers_or_volunteering_service.csv':\n",
    "            string_to_add = \", 4\"\n",
    "        elif finput == 'caution_and_advice.csv':\n",
    "            string_to_add = \", 5\"\n",
    "        elif finput == 'sympathy_and_emotional_support.csv':\n",
    "            string_to_add = \", 6\"\n",
    "        elif finput == 'other_useful_information.csv':\n",
    "            string_to_add = \", 7\"\n",
    "        elif finput == 'not_related_or_irrelevant.csv':\n",
    "            string_to_add = \", 8\"\n",
    "        else:\n",
    "            string_to_add = \", 9\"\n",
    "\n",
    "        file_lines = [''.join([x.strip(), string_to_add, '\\n']) for x in fin.readlines()]\n",
    "        \n",
    "        with open(( finput.rsplit( \".\", 1 )[ 0 ] ) + \".txt\", \"w+\") as fout:\n",
    "            fout.write('tweet_text,label\\n')\n",
    "            fout.writelines(file_lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Equisample lines from all classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "path = '/home/user/elicon/data/Training_Data/Binary_Classification_Data/Nepal_Quake'\n",
    "os.chdir(path)\n",
    "dest_file = 'train.csv'\n",
    "\n",
    "input_files = glob.glob(\"*.txt\")\n",
    "\n",
    "with open(dest_file, \"a\") as sink:\n",
    "    sink.write('tweet_text,label\\n')\n",
    "    for finput in input_files:\n",
    "        with open(finput, 'r') as fin:\n",
    "            lines = [line for line in fin]\n",
    "            random_choice = random.sample(lines, 2800)            \n",
    "            sink.write(\"\".join(random_choice))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add labels for binary classification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#path = '/home/user/elicon/data/Training_Data/Binary_Classification_Data/Nepal_Quake'\n",
    "path = '/home/user/elicon/data'\n",
    "os.chdir(path)\n",
    "\n",
    "input_files = glob.glob(\"*.csv1\")\n",
    "for finput in input_files: #iterates over all files in the directory ending in .csv\n",
    "    with open(finput, 'r') as fin:\n",
    "        if finput == 'negative.csv1':\n",
    "            string_to_add = \", 0\"\n",
    "        elif finput == 'positive.csv1':\n",
    "            string_to_add = \", 1\"\n",
    "        else:\n",
    "            string_to_add = \", 2\"\n",
    "        file_lines = [''.join([x.strip(), string_to_add, '\\n']) for x in fin.readlines()]\n",
    "        \n",
    "        with open(( finput.rsplit( \".\", 1 )[ 0 ] ) + \".txt\", \"w+\") as fout:\n",
    "            #fout.write('tweet_text,label\\n')\n",
    "            fout.writelines(file_lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Equisample lines form both positive and negative class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "sample larger than population",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-5d76c08d7ca4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mline\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfin\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m             \u001b[0mrandom_choice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlines\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m400\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m             \u001b[0msink\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_choice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/user/anaconda2/lib/python2.7/random.pyc\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, population, k)\u001b[0m\n\u001b[1;32m    321\u001b[0m         \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpopulation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 323\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"sample larger than population\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    324\u001b[0m         \u001b[0mrandom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m         \u001b[0m_int\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: sample larger than population"
     ]
    }
   ],
   "source": [
    "import random\n",
    "#path = '/home/user/elicon/data/Training_Data/Binary_Classification_Data/Nepal_Quake'\n",
    "#path = '/home/user/elicon/data/Test_Data/California_Quake/datasets/'\n",
    "path = '/home/user/elicon/data'\n",
    "os.chdir(path)\n",
    "dest_file = 'train.csv'\n",
    "\n",
    "input_files = glob.glob(\"*.txt\")\n",
    "\n",
    "with open(dest_file, \"a\") as sink:\n",
    "    sink.write('tweet_text,label\\n')\n",
    "    for finput in input_files:\n",
    "        with open(finput, 'r') as fin:\n",
    "            lines = [line for line in fin]\n",
    "            random_choice = random.sample(lines, 400)            \n",
    "            sink.write(\"\".join(random_choice))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import time\n",
    "import string\n",
    "\n",
    "tweets_data_path = r'/home/user/Gmail/nepal_eq/json/idinfrastructure_and_utilities_damage.json'\n",
    "tweets_dest = r'/home/user/elicon/data/Training_Data/Nepal_Quake/infrastructure_and_utilities_damage.csv'\n",
    "tweets_data = []\n",
    "with open(tweets_data_path, \"r\") as tweets_file:\n",
    "    with open(tweets_dest,'w+') as fout:\n",
    "        for line in tweets_file:\n",
    "            try:\n",
    "                tweet = json.loads(line)\n",
    "                id = tweet['id']\n",
    "                if 'retweeted_status' in tweet:\n",
    "                    text = tweet['retweeted_status']['text']\n",
    "                else:\n",
    "                    text = tweet['text']\n",
    "                fout.write(str(id) + \",\" + str(text) + \"\\n\")\n",
    "            except:\n",
    "                continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create training datasets as pairs tweet and label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## create the tweet label file\n",
    "original_file = \"../../data/California-Earthquake/califor_neg.csv\"\n",
    "tweet_file = \"../../data/California-Earthquake/califor_neg_sem.csv\"\n",
    "output_file = \"../../data/California-Earthquake/califor_neg_train.csv\"\n",
    "!cat $original_file | cut -d , -f2 > $tweet_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "filtr_file = \"../../data/California-Earthquake/califor_neg_filtr.csv\"\n",
    "with open(tweet_file, \"r\") as fin,open(filtr_file, \"w+\") as fout:\n",
    "    for tweet in fin:\n",
    "        clean_doc = ' '.join(split_text(tweet))\n",
    "        json.dump(clean_doc, fout)\n",
    "        fout.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "h\n",
    "\n",
    "!rm $tweet_file\n",
    "!rm $filtr_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Add a header to the training file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv, glob\n",
    "\n",
    "path = '/home/user/elicon/data/Training_Data/Nepal_Quake/labeled'\n",
    "os.chdir(path)\n",
    "#os.getcwd()\n",
    "input_files = glob.glob(\"*.csv\")\n",
    "for training_file in input_files: #iterates over all files in the directory ending in .csv\n",
    "    with open(training_file, \"wb\") as train_file:\n",
    "        writer = csv.writer(train_file)\n",
    "        writer.writerow([\"tweet_text\", \"label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shuffle lines to get the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(dest_file, \"rb\") as source:\n",
    "    lines = [line for line in source]\n",
    "import random\n",
    "random_choice = random.sample(lines, 200)\n",
    "with open(training_file, \"a\") as sink:\n",
    "    sink.write(\"\".join(random_choice))\n",
    "!rm $dest_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Extract time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "date = tweet['created_at']\n",
    "try:\n",
    "    c = time.strptime(tweet_gmttime.replace(\"+0000\",''), '%a %b %d %H:%M:%S %Y')\n",
    "except: \n",
    "    print \"pb with tweet_gmttime\", tweet_gmttime, line\n",
    "    pass\n",
    "\n",
    "tweet_unixtime = int(time.mktime(c))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
