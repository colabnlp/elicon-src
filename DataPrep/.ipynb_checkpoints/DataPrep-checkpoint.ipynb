{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "link = 'http://crisisnlp.qcri.org/data/lrec2016/labeled_cf/CrisisNLP_labeled_data_crowdflower_V1.2.zip'\n",
    "dest = '../../data/Datasets'\n",
    "datasets = 'CrisisNLP_labeled_data_crowdflower_V1.2.zip'    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!cd $dest && wget $link && unzip $datasets && rm -r $datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "original_file = '2014_california_eq.csv'\n",
    "original_folder = '../../data/Datasets/CrisisNLP_labeled_data_crowdflower.DS_Store/2014_California_Earthquake'\n",
    "dest_folder = '../../data/Datasets/Training_Data/California-Quake'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!cd $dest_folder && awk -F ',' '{print>$1}' $original_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!cd $original_folder && awk -F ',' '{print > (\"data\"$0\".csv\")}' $original_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!cat $original_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!java -classpath TweetsRetrieval-1.2-jar-with-dependencies.jar qa.qcri.tweetsretrieval.TweetsRetrievalTool $original_file "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import some nltk functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import csv\n",
    "import string\n",
    "import re\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import *\n",
    "stemmer = PorterStemmer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def expand_around_chars(text, characters):\n",
    "    for char in characters:\n",
    "        text = text.replace(char, ' ' + char + ' ')\n",
    "    return text\n",
    "\n",
    "def strip_quotations_newline(text):\n",
    "    clean_tweet = ' '.join(text.split())\n",
    "    clean_tweet = clean_tweet.decode('unicode_escape').encode('ascii','ignore')\n",
    "    clean_tweet = clean_tweet.replace('\",\\'','')\n",
    "    clean_tweet = re.sub(r'''(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\".,<>?«»“”‘’]))''', \"\", clean_tweet)\n",
    "    clean_tweet = re.sub(r'''(@[A-Za-z0-9]+)''', \"\", clean_tweet)\n",
    "    clean_tweet = re.sub(\"([0-9]+)\", \"\", clean_tweet)\n",
    "    return clean_tweet\n",
    "\n",
    "def split_text(text):\n",
    "    text = strip_quotations_newline(text)\n",
    "    text = expand_around_chars(text, '\\/\".,()&[]{}:;!-_\\'')\n",
    "    splitted_text = text.split(' ')\n",
    "    cleaned_text = [x for x in splitted_text if len(x) > 1]\n",
    "    text_lowercase = [x.lower() for x in cleaned_text]\n",
    "    text_stemmed = [stemmer.stem(x) for x in text_lowercase]\n",
    "    text_stemmed = [x.decode('unicode_escape').encode('ascii','ignore') for x in text_stemmed]\n",
    "    text_filtered = [x for x in text_stemmed if not x in stop_words]    \n",
    "    return text_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JSON to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "indir = r'~/elicon/data/Test1'\n",
    "tweets_dest = r'~/elicon/data/Training_Data/California_Quake/'\n",
    "for root, dirs, filenames in os.walk(indir):\n",
    "    for f in filenames:\n",
    "        with open(os.path.join(root, f), 'r') as tweets_file\n",
    "        with open(tweets_dest+f,'w+') as fout\n",
    "        for line in tweets_file:\n",
    "            try:\n",
    "                tweet = json.loads(line)\n",
    "                id = tweet['id']\n",
    "                text = tweet['text']\n",
    "                fout.write(str(id) + \",\" + str(text) + \"\\n\")\n",
    "            except:\n",
    "                continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "path = '/home/user/elicon/data/Training_Data/Nepal_Quake'\n",
    "os.chdir(path)\n",
    "os.getcwd()\n",
    "input_files = glob.glob(\"*.csv\")\n",
    "for finput in input_files: #iterates over all files in the directory ending in .csv\n",
    "    with open(finput) as fin:\n",
    "        with open(( finput.rsplit( \".\", 1 )[ 0 ] ) + \".txt\", \"w+\") as fout:\n",
    "            for tweet in fin:\n",
    "                clean_doc = ' '.join(split_text(tweet))\n",
    "                json.dump(clean_doc, fout)\n",
    "                fout.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## add labels \n",
    "path = '/home/user/elicon/data/Training_Data/Nepal_Quake/labeled'\n",
    "os.chdir(path)\n",
    "#os.getcwd()\n",
    "input_files = glob.glob(\"*.csv\")\n",
    "for finput in input_files: #iterates over all files in the directory ending in .csv\n",
    "    with open(finput, 'r') as fin:\n",
    "        if finput == 'injured_or_dead_people.csv':\n",
    "            string_to_add = \", 0\"\n",
    "        elif finput == 'missing_trapped_or_found_people.csv':\n",
    "            string_to_add = \", 1\"\n",
    "        elif finput == 'displaced_people_and_evacuations.csv':\n",
    "            string_to_add = \", 2\"\n",
    "        elif finput == 'infrastructure_and_utilities_damage.csv':\n",
    "            string_to_add = \", 3\"\n",
    "        elif finput == 'donation_needs_or_offers_or_volunteering_service.csv':\n",
    "            string_to_add = \", 4\"\n",
    "        elif finput == 'caution_and_advice.csv':\n",
    "            string_to_add = \", 5\"\n",
    "        elif finput == 'sympathy_and_emotional_support.csv':\n",
    "            string_to_add = \", 6\"\n",
    "        elif finput == 'other_useful_information.csv':\n",
    "            string_to_add = \", 7\"\n",
    "        elif finput == 'not_related_or_irrelevant.csv':\n",
    "            string_to_add = \", 8\"\n",
    "        else:\n",
    "            string_to_add = \", 9\"\n",
    "\n",
    "        file_lines = [''.join([x.strip(), string_to_add, '\\n']) for x in fin.readlines()]\n",
    "        \n",
    "        with open(( finput.rsplit( \".\", 1 )[ 0 ] ) + \".txt\", \"w+\") as fout:\n",
    "            fout.writelines(file_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import time\n",
    "import string\n",
    "\n",
    "tweets_data_path = r'/home/user/Gmail/pak_eq/json/idother_useful_information.json'\n",
    "tweets_dest = r'/home/user/elicon/data/Training_Data/Pakistan_Quake/other_useful_information.csv'\n",
    "tweets_data = []\n",
    "with open(tweets_data_path, \"r\") as tweets_file:\n",
    "    with open(tweets_dest,'w+') as fout:\n",
    "        for line in tweets_file:\n",
    "            try:\n",
    "                tweet = json.loads(line)\n",
    "                id = tweet['id']\n",
    "                if 'retweeted_status' in tweet:\n",
    "                    text = tweet['retweeted_status']['text']\n",
    "                else:\n",
    "                    text = tweet['text']\n",
    "                fout.write(str(id) + \",\" + str(text) + \"\\n\")\n",
    "            except:\n",
    "                continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create training datasets as pairs tweet and label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## create the tweet label file\n",
    "original_file = \"../../data/California-Earthquake/califor_neg.csv\"\n",
    "tweet_file = \"../../data/California-Earthquake/califor_neg_sem.csv\"\n",
    "output_file = \"../../data/California-Earthquake/califor_neg_train.csv\"\n",
    "!cat $original_file | cut -d , -f2 > $tweet_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "filtr_file = \"../../data/California-Earthquake/califor_neg_filtr.csv\"\n",
    "with open(tweet_file, \"r\") as fin,open(filtr_file, \"w+\") as fout:\n",
    "    for tweet in fin:\n",
    "        clean_doc = ' '.join(split_text(tweet))\n",
    "        json.dump(clean_doc, fout)\n",
    "        fout.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "h\n",
    "\n",
    "!rm $tweet_file\n",
    "!rm $filtr_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Equisample lines form both positive and negative class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "source_file = \"../../data/California-Earthquake/califor_pos_train.csv\"\n",
    "dest_file = \"../../data/California-Earthquake/califor_tot.csv\"\n",
    "with open(source_file, \"rb\") as source:\n",
    "    lines = [line for line in source]\n",
    "import random\n",
    "random_choice = random.sample(lines, 100)\n",
    "with open(dest_file, \"a\") as sink:\n",
    "    sink.write(\"\".join(random_choice))\n",
    "!rm $source_file    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Add a header to the training file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "training_file = \"../../data/California-Earthquake/califor_train.csv\"\n",
    "\n",
    "with open(training_file, \"wb\") as train_file:\n",
    "    writer = csv.writer(train_file)\n",
    "    writer.writerow([\"tweet_text\", \"label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shuffle lines to get the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(dest_file, \"rb\") as source:\n",
    "    lines = [line for line in source]\n",
    "import random\n",
    "random_choice = random.sample(lines, 200)\n",
    "with open(training_file, \"a\") as sink:\n",
    "    sink.write(\"\".join(random_choice))\n",
    "!rm $dest_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
