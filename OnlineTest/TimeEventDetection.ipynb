{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataframe = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = mags.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pandas import Series\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "series = Series.from_csv('/home/user/elicon/data/mars-earthquakes.csv', header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "time\n",
       "2018-03-02 15:25:23.220    36.582000\n",
       "2018-03-02 16:55:04.850    36.037167\n",
       "2018-03-02 20:22:13.600    40.466500\n",
       "2018-03-03 07:00:12.670    33.477333\n",
       "2018-03-03 11:04:04.620    37.846667\n",
       "2018-03-05 13:54:41.520    40.160167\n",
       "2018-03-05 22:19:44.940    38.778000\n",
       "2018-03-06 12:38:16.730    37.001667\n",
       "2018-03-06 12:44:06.150    37.004167\n",
       "2018-03-06 17:41:28.950    34.391167\n",
       "2018-03-06 22:21:26.720    37.463167\n",
       "2018-03-07 04:26:43.510    37.006333\n",
       "2018-03-07 18:05:00.420    35.487167\n",
       "2018-03-08 08:08:29.440    35.558500\n",
       "2018-03-09 04:21:04.220    39.295500\n",
       "2018-03-09 05:11:49.240    35.799333\n",
       "2018-03-09 06:01:28.350    40.291833\n",
       "2018-03-10 01:21:24.930    40.550167\n",
       "2018-03-10 15:25:33.970    37.416000\n",
       "2018-03-11 09:42:46.810    40.303667\n",
       "2018-03-11 12:35:42.070    35.832000\n",
       "2018-03-11 12:45:22.130    33.177833\n",
       "2018-03-11 18:09:08.410    36.943667\n",
       "2018-03-11 18:10:35.830    36.943167\n",
       "2018-03-12 00:52:10.580    40.371333\n",
       "2018-03-13 05:59:06.440    39.749667\n",
       "2018-03-13 09:57:04.350    35.603167\n",
       "2018-03-14 12:27:53.950    35.942167\n",
       "2018-03-14 13:26:50.180    33.952500\n",
       "2018-03-14 15:41:23.970    40.396500\n",
       "                             ...    \n",
       "2018-03-20 19:32:26.970    40.438000\n",
       "2018-03-21 13:50:39.410    40.814835\n",
       "2018-03-21 21:11:31.290    36.858000\n",
       "2018-03-22 03:57:35.540    40.290833\n",
       "2018-03-22 16:24:49.560    40.751333\n",
       "2018-03-22 19:27:36.340    36.006000\n",
       "2018-03-22 22:57:55.810    36.232833\n",
       "2018-03-23 03:09:39.100    40.427833\n",
       "2018-03-23 03:12:21.150    40.401500\n",
       "2018-03-23 03:18:50.370    40.450333\n",
       "2018-03-23 04:15:58.720    40.418667\n",
       "2018-03-23 08:49:25.840    35.396500\n",
       "2018-03-23 14:30:31.430    34.106833\n",
       "2018-03-24 05:09:03.910    40.404833\n",
       "2018-03-24 20:08:49.620    34.065167\n",
       "2018-03-24 21:29:50.170    33.261667\n",
       "2018-03-24 22:14:48.730    41.182833\n",
       "2018-03-25 00:51:49.050    36.415167\n",
       "2018-03-25 18:59:33.780    38.802333\n",
       "2018-03-26 06:52:31.660    36.004000\n",
       "2018-03-26 08:08:15.340    41.001667\n",
       "2018-03-26 20:58:58.020    38.377499\n",
       "2018-03-26 22:22:06.270    37.836333\n",
       "2018-03-27 16:27:07.140    38.210000\n",
       "2018-03-27 22:54:41.190    40.513833\n",
       "2018-03-30 06:39:56.380    36.988333\n",
       "2018-03-30 15:05:39.740    37.980333\n",
       "2018-03-31 09:05:53.260    37.058666\n",
       "2018-03-31 14:33:39.330    36.860332\n",
       "2018-03-31 15:07:31.240    36.861999\n",
       "Name: latitude, dtype: float64"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "series.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f2674d327d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "series.hist()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    2018-03-31T15:07:31.240Z\n",
      "1    2018-03-31T14:33:39.330Z\n",
      "2    2018-03-31T09:05:53.260Z\n",
      "3    2018-03-30T15:05:39.740Z\n",
      "4    2018-03-30T06:39:56.380Z\n",
      "5    2018-03-27T22:54:41.190Z\n",
      "6    2018-03-27T16:27:07.140Z\n",
      "Name: time, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(mags.head(7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Csv to time-event dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = spark.read.option(\"header\",\"true\").csv(\"/home/user/elicon/data/mars-earthquakes.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- time: string (nullable = true)\n",
      " |-- latitude: string (nullable = true)\n",
      " |-- longitude: string (nullable = true)\n",
      " |-- depth: string (nullable = true)\n",
      " |-- mag: string (nullable = true)\n",
      " |-- magType: string (nullable = true)\n",
      " |-- nst: string (nullable = true)\n",
      " |-- gap: string (nullable = true)\n",
      " |-- dmin: string (nullable = true)\n",
      " |-- rms: string (nullable = true)\n",
      " |-- net: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- updated: string (nullable = true)\n",
      " |-- place: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- horizontalError: string (nullable = true)\n",
      " |-- depthError: string (nullable = true)\n",
      " |-- magError: string (nullable = true)\n",
      " |-- magNst: string (nullable = true)\n",
      " |-- status: string (nullable = true)\n",
      " |-- locationSource: string (nullable = true)\n",
      " |-- magSource: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mags = df.select('time', 'mag')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[time: string, mag: string]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#saved pipeline model\n",
    "from pyspark.ml import PipelineModel\n",
    "pipeModel = PipelineModel.load(\"/home/user/elicon/src/Training/target/tmp/HashingTF_Binary_pipeline_model_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#saved training model\n",
    "from pyspark.ml.classification import *\n",
    "trainModel = LogisticRegressionModel.load(\"/home/user/elicon/src/Training/target/tmp/logisticRegression_Binary_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tfidfData = pipeModel.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trained_data = trainModel.transform(tfidfData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import from_unixtime\n",
    "spark.conf.set(\"spark.sql.session.timeZone\", \"America/Los_Angeles\")\n",
    "trained_data.select(from_unixtime('time').alias('time'),'text',trained_data.prediction.cast('string').alias('action')).coalesce(1).write.mode('overwrite').option('header', 'true').json('/home/user/tweets-test.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch/Interactive Processing\n",
    "\n",
    "The usual first step in attempting to process the data is to interactively query the data. Let's define a static DataFrame on the files, and give it a table name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trained_data.select(from_unixtime('time').alias('time'),trained_data.prediction.cast('string').alias('action')).coalesce(1).write.mode('overwrite').option('header', 'true').json('/home/user/tweets-test.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "inputPath = '/home/user/tweets-test.json' \n",
    "# Since we know the data format already, let's define the schema to speed up processing (no need for Spark to infer schema)\n",
    "jsonSchema = StructType([ StructField(\"time\", TimestampType(), True), StructField(\"action\", StringType(), True) ])\n",
    "\n",
    "# Static DataFrame representing data in the JSON files\n",
    "staticInputDF = (\n",
    "  spark\n",
    "    .read\n",
    "    .schema(jsonSchema)\n",
    "    .json(inputPath)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *      # for window() function\n",
    "\n",
    "staticCountsDF = (\n",
    "  staticInputDF\n",
    "    .groupBy(\n",
    "       staticInputDF.action, \n",
    "       window(staticInputDF.time, \"1 hour\"))    \n",
    "    .count()\n",
    ")\n",
    "staticCountsDF.cache()\n",
    "\n",
    "# Register the DataFrame as table 'static_counts'\n",
    "staticCountsDF.createOrReplaceTempView(\"static_counts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "total_counts = staticCountsDF.groupBy('action').sum('count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+\n",
      "|action|sum(count)|\n",
      "+------+----------+\n",
      "|   1.0|      8612|\n",
      "|   0.0|       196|\n",
      "+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "total_counts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------+\n",
      "|time               |action|\n",
      "+-------------------+------+\n",
      "|null               |1.0   |\n",
      "|2018-02-21 17:40:13|1.0   |\n",
      "|2018-02-21 17:41:05|1.0   |\n",
      "|2018-02-21 17:41:12|1.0   |\n",
      "|2018-02-21 17:41:28|1.0   |\n",
      "|2018-02-21 17:41:32|0.0   |\n",
      "|2018-02-21 17:41:32|1.0   |\n",
      "|2018-02-21 17:42:12|1.0   |\n",
      "|2018-02-21 17:42:28|1.0   |\n",
      "|2018-02-21 17:42:49|1.0   |\n",
      "|2018-02-21 17:43:03|0.0   |\n",
      "|2018-02-21 17:43:43|1.0   |\n",
      "|2018-02-21 17:43:45|1.0   |\n",
      "|2018-02-21 17:43:51|1.0   |\n",
      "|2018-02-21 17:44:46|1.0   |\n",
      "|2018-02-21 17:44:46|1.0   |\n",
      "|2018-02-21 17:44:58|1.0   |\n",
      "|2018-02-21 17:45:02|1.0   |\n",
      "|2018-02-21 17:45:35|1.0   |\n",
      "|2018-02-21 17:45:43|1.0   |\n",
      "+-------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "staticInputDF.orderBy('time','action').show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Preprocessing with SparkNLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sparknlp.annotator import *\n",
    "from sparknlp.common import *\n",
    "from sparknlp.base import *\n",
    "from pyspark.ml import Pipeline\n",
    "### Define the dataframe                                                               \n",
    "document_assembler = DocumentAssembler() \\\n",
    "            .setInputCol(\"text\")\n",
    "assembled = document_assembler.transform(tweet_data)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assembled.select('document').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer() \\\n",
    "  .setInputCols([\"document\"]) \\\n",
    "  .setOutputCol(\"token\") \\\n",
    "  #.addInfixPattern(\"(\\p{L}+)(n't\\b)\")\n",
    "tokenized = tokenizer.transform(assembled)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenized.select('token').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "normalizer = Normalizer() \\\n",
    "  .setInputCols([\"token\"]) \\\n",
    "  .setOutputCol(\"normalized\") \\\n",
    "  .setPattern(r'''(?i)\\b((?:https?:\\/\\/|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\".,<>?«»“”‘’]))''')\n",
    "normalized = normalizer.transform(tokenized)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "normalized.select('normalized').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stemmer = Stemmer() \\\n",
    "  .setInputCols([\"normalized\"]) \\\n",
    "  .setOutputCol(\"stem\")\n",
    "stemmed = stemmer.transform(normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stemmed.select('stem').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "finisher = Finisher() \\\n",
    "    .setInputCols([\"stem\"]) \\\n",
    "    .setIncludeKeys(True) \\\n",
    "    .setCleanAnnotations(True)\n",
    "finished_data = finisher.transform(stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[\n",
    "    document_assembler,\n",
    "    tokenizer,\n",
    "    normalizer,\n",
    "    stemmer,\n",
    "    finisher\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spelled_data = pipeline.fit(tweet_data).transform(tweet_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spelled_data.select('finished_stem').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spelled_data.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Tokenise tweets\n",
    "from pyspark.ml.feature import Tokenizer\n",
    "#from pyspark.sql.functions import col, udf\n",
    "#from pyspark.sql.types import IntegerType\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"finished_stem\", outputCol=\"words\")\n",
    "#countTokens = udf(lambda words: len(words), IntegerType())\n",
    "tokenized = tokenizer.transform(finished_data)\n",
    "#tokenized = tokenized.withColumn(\"tokens\", countTokens(col(\"words\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokenized.select('words').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover\n",
    "# stop words\n",
    "add_stopwords = [\"amp\",\"rt\",\"ca\",\"pg\",\"ga\",\"cb\",\"ap\",\"sce\",\"st\"] # standard stop words\n",
    "stopwordsRemover = StopWordsRemover(inputCol=\"finished_stem\", outputCol=\"filtered\").setStopWords(add_stopwords)\n",
    "stoped = stopwordsRemover.transform(finished_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 3-Gram for context feature\n",
    "from pyspark.ml.feature import NGram\n",
    "\n",
    "ngram = NGram(n=3, inputCol=\"filtered\", outputCol=\"ngrams\")\n",
    "#ngrams = ngram.transform(stoped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use HashingTF Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hashing TF feature extraction\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "from pyspark.ml.linalg import Vector as MLVector, Vectors as MLVectors\n",
    "from pyspark.mllib.linalg import Vector as MLLibVector, Vectors as MLLibVectors\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "hashingTF = HashingTF(inputCol=\"finished_stem\", outputCol=\"rawFeatures\", numFeatures=50000)\n",
    "featurizedData = hashingTF.transform(finished_data)\n",
    "#idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "featurizedData.select('rawFeatures').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "#pipeline = Pipeline(stages=[tokenizer,stopwordsRemover, ngram, word2Vec])# Fit the pipeline to training documents.\n",
    "pipeline = Pipeline(stages=[document_assembler, tokenizer, normalizer, stemmer, finisher, stopwordsRemover, ngram, hashingTF, idf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipelineFit = pipeline.fit(tweet_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "jsonSchema = StructType([ StructField(\"time\", TimestampType(), True), StructField(\"trigger\", StringType(), True) ])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
