{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!pip install pymongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#    Spark\n",
    "from pyspark import SparkContext\n",
    "#    Spark Streaming\n",
    "from pyspark.streaming import StreamingContext\n",
    "#    Kafka\n",
    "from pyspark.streaming.kafka import KafkaUtils\n",
    "#    json parsing\n",
    "import json\n",
    "#    PyMongo\n",
    "import pymongo\n",
    "#    Time\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'created_at_1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MONGODB_URI = \"mongodb://localhost:27017/query2\"\n",
    "tweets_collection = \"predictions\"\n",
    "\n",
    "client = pymongo.MongoClient(MONGODB_URI)\n",
    "parsed_dburi = pymongo.uri_parser.parse_uri(MONGODB_URI)\n",
    "#       Database\n",
    "db = client[parsed_dburi['database']]\n",
    "\n",
    "#       Collection\n",
    "tweets_col = db[tweets_collection]\n",
    "#       Index\n",
    "#tweets.create_index(\"tweet_text\", unique=True)\n",
    "tweets_col.create_index(\"created_at\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Streaming Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-0385c3146e1c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mwindow_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m15\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbatch_interval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mfrequency\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m6\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbatch_interval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mssc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStreamingContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_interval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mtweets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKafkaUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateStream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mssc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzkQuorum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"spark-streaming-consumer\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mtopic\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sc' is not defined"
     ]
    }
   ],
   "source": [
    "zkQuorum = \"localhost:2181\"\n",
    "topic = \"twitter-stream\"\n",
    "batch_interval = 1\n",
    "window_length = 15 * batch_interval\n",
    "frequency = 6 * batch_interval\n",
    "ssc = StreamingContext(sc, batch_interval)\n",
    "\n",
    "tweets = KafkaUtils.createStream(ssc, zkQuorum, \"spark-streaming-consumer\", {topic: 1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse the inbound message as json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tweets' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-0b6ad7eecd94>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mparsed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtweets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m#parsed = parsed.filter(lambda tweet: tweet['geo']['coordinates'] != None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#parsed.filter(lambda tweet: tweet['metadata']['place']['country'] == 'United States')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mparsed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'Tweets in this batch: %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tweets' is not defined"
     ]
    }
   ],
   "source": [
    "parsed = tweets.map(lambda v: json.loads(v[1]))\n",
    "#parsed = parsed.filter(lambda tweet: tweet['geo']['coordinates'] != None)\n",
    "#parsed.filter(lambda tweet: tweet['metadata']['place']['country'] == 'United States')\n",
    "parsed.count().map(lambda x:'Tweets in this batch: %s' % x).pprint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Extract Text from each tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text_dstream = parsed.map(lambda tweet: tweet['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process the extracted text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import csv\n",
    "import string\n",
    "import re\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def filter_out_unicode(tweet):\n",
    "  \n",
    "    try:\n",
    "        clean_tweet = str(tweet)\n",
    "    except UnicodeEncodeError:\n",
    "        pass\n",
    "    return clean_tweet\n",
    "\n",
    "def expand_around_chars(text, characters):\n",
    "    for char in characters:\n",
    "        text = text.replace(char, ' ' + char + ' ')\n",
    "    return text\n",
    "\n",
    "def strip_quotations_newline(text):\n",
    "    clean_tweet = ' '.join(text.split())\n",
    "    clean_tweet = clean_tweet.encode('utf-8')\n",
    "    clean_tweet = clean_tweet.replace('\",\\'','')\n",
    "    clean_tweet = re.sub(r'''(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\".,<>?«»“”‘’]))''', \"\", clean_tweet)\n",
    "    clean_tweet = re.sub(r'''(@[A-Za-z0-9]+)''', \"\", clean_tweet)\n",
    "    clean_tweet = re.sub(\"([0-9]+)\", \"\", clean_tweet)\n",
    "    clean_tweet = re.sub(r'[^\\x00-\\x7F]+','', clean_tweet)\n",
    "    return clean_tweet\n",
    "\n",
    "def split_text(text):\n",
    "    text = strip_quotations_newline(text)\n",
    "    text = expand_around_chars(text, '\\/\".,()&[]{}:;!-_\\'')\n",
    "    splitted_text = text.split(' ')\n",
    "    cleaned_text = [x for x in splitted_text if len(x) > 2]\n",
    "    text_lowercase = [x.lower() for x in cleaned_text]\n",
    "    return text_lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mess = text_dstream.map(lambda text: ' '.join(split_text(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.classification import SVMModel\n",
    "from pyspark.ml import PipelineModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import linalg as ml_linalg\n",
    "\n",
    "def as_mllib(v):\n",
    "    if isinstance(v, ml_linalg.SparseVector):\n",
    "        return MLLibVectors.sparse(v.size, v.indices, v.values)\n",
    "    elif isinstance(v, ml_linalg.DenseVector):\n",
    "        return MLLibVectors.dense(v.toArray())\n",
    "    else:\n",
    "        raise TypeError(\"Unsupported type: {0}\".format(type(v)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "path = '/home/user/elicon/src/Training/'\n",
    "os.chdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql import Row, SparkSession, DataFrameWriter\n",
    "from pyspark.ml.feature import Tokenizer\n",
    "from pyspark.sql.functions import col, monotonically_increasing_id\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "from pyspark.ml.feature import HashingTF, IDF\n",
    "from pyspark.ml.linalg import Vector as MLVector, Vectors as MLVectors\n",
    "from pyspark.mllib.linalg import Vector as MLLibVector, Vectors as MLLibVectors\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "def getSparkSessionInstance(sparkConf):\n",
    "    if ('sparkSessionSingletonInstance' not in globals()):\n",
    "        globals()['sparkSessionSingletonInstance'] = SparkSession\\\n",
    "            .builder\\\n",
    "            .config(conf=sparkConf)\\\n",
    "            .getOrCreate()\n",
    "    return globals()['sparkSessionSingletonInstance']\n",
    "\n",
    "words = mess\n",
    "# Convert RDDs of the words DStream to DataFrame and run SQL query\n",
    "def process(time, rdd):\n",
    "\n",
    "    # Get the singleton instance of SparkSession\n",
    "    if not(rdd.isEmpty() and rdd != None):\n",
    "        spark = getSparkSessionInstance(rdd.context.getConf())\n",
    "\n",
    "        # Convert RDD[String] to RDD[Row] to DataFrame\n",
    "        rowRdd = rdd.map(lambda w: Row(tweet_text=w))\n",
    "        wordsDataFrame = spark.createDataFrame(rowRdd)\n",
    "\n",
    "        # Creates a temporary view using the DataFrame.\n",
    "        wordsDataFrame.createOrReplaceTempView(\"tweets\")\n",
    "\n",
    "        # Do word count on table using SQL and print it\n",
    "        wordCountsDataFrame = \\\n",
    "            spark.sql(\"select tweet_text from tweets\")\n",
    "        pipeModel = PipelineModel.load(\"target/tmp/HashingTF_Binary_model\")\n",
    "        # Extract features\n",
    "        tfidfData = pipeModel.transform(wordsDataFrame)\n",
    "        #tfidfData.show()\n",
    "        doo = tfidfData.select(\"features\").rdd\n",
    "        doo_data = doo.map(lambda y: LabeledPoint(0, as_mllib(y[0])))\n",
    "        doo_df = spark.createDataFrame(doo_data)\n",
    "        # Trained model \n",
    "        sameModel = SVMModel.load(sc, \"target/tmp/SVMWithSGD_Binary_model\")\n",
    "        doo_lab = doo_data.map(lambda p: (p.label, sameModel.predict(p.features)))\n",
    "        doo_label = spark.createDataFrame(doo_lab)\n",
    "        fin = doo_label.selectExpr(\"_1 as label\", \"_2 as prediction\")\n",
    "        preds = fin.select(\"prediction\")\n",
    "        text = wordCountsDataFrame.withColumn(\"id\", monotonically_increasing_id())\n",
    "        preds = preds.withColumn(\"id\", monotonically_increasing_id())\n",
    "        text_preds = preds.join(text, \"id\", \"outer\").drop(\"id\")\n",
    "        \n",
    "        ## Running not removing duplicates though\n",
    "        #text_preds.dropDuplicates(['tweet_text'])\n",
    "        \n",
    "        text_preds.write.format(\"com.stratio.datasource.mongodb\").mode('append').options(host='localhost:27017',database='query2', collection='predictions').save()\n",
    "        #tweets_col.update_many({}, { '$set' : { \"created_at\" : datetime.datetime.utcnow() } }, True, True)\n",
    "        \n",
    "        #n_0 = text_preds.filter(text_preds.prediction != 1).count()\n",
    "        text_preds.show(40, truncate=False)\n",
    "\n",
    "words.foreachRDD(process)\n",
    "ssc.start()\n",
    "ssc.awaitTermination(timeout=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
